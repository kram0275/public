---
title: "Material Performance Optimization via Multiple Regression"
author: "Will Kramlinger"
output: 
  html_document:
    code_folding: hide
    toc: TRUE
    toc_depth: 2
    toc_float: TRUE
    fig_caption: true
    theme: spacelab
---
<style>
body {
text-align: justify}
</style>
```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem
A formulation was sought which optimized the performance (**perf**) of a prototype material. Via background knowledge of the experimenter and department, the most efficient way of tweaking the performance was adjustment of a ratio (**ratio**) between two raw component materials. Thus, an optimal value of **ratio** was sought which maximized **perf**.

## Data

Towards this end, an experiment was designed that consisted of measuring the response **perf** of 4 different samples with evenly spaced values of **ratio**.   Due to time and material constraints, only 1 replicate per sample was performed. The original data set is shown in Table 1; a plot of **perf** vs. **ratio** is shown as Fig. 1.

```{r Table 1: Summary, message = FALSE, warning = FALSE}
rm(list = ls())

library(knitr)
library(kableExtra)
library(ggplot2)

df <- read.csv(file = "dummy_data.csv")
colnames(df) <- c("ratio", "perf")

attach(df)

df %>%
  kable(align = "l") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = TRUE, font_size = 12, position = "center") %>% 
  scroll_box(width = "100%", height = "160px")
```
*Table 1: Original data set comprising 4 observations of material performance, each with a different value of ratio (predictor).*  

<center>
```{r Fig 1. Perf vs Ratio, echo = FALSE, fig.cap = "Fig. 1: Dependence of material performance on raw material ratio. Plot resembles a parabola and strongly suggests usage of polynomial regression to produce a line of best fit.", out.width = '60%'}

ggplot(df, aes(x = ratio, y = perf)) +
  geom_point(colour = "blue") +
  theme_bw() +
  xlab('Raw Material Ratio, "Ratio"') + 
  ylab('Material Performance, "Perf"')


```
</center>
  
Upon visual inspection of Fig. 1, a curvilinear relationship is observed between the two variables; specifically, the data appears parabolic. Based on previous experience, the parabolic nature of the data fully corroborated the experimenter's expectations and intuition that a multiple regression model would need to be implemented.  
  
  
## OLS Regression

Ordinary least squares (OLS) multiple regression was conducted on the data set according to the model  

<center>  
  
$perf_{i} = \beta_{0} + \beta_{1}ratio_{i} + \beta_{2}ratio_{i}^2 $.  [Eq. 1]  
  
</center>
  
The regression output is shown below as Table 3.  


```{r  Table 3: Regression Output, message = FALSE}
poly.scaled <- lm(perf ~ poly(ratio, 2), data = df)
load(file = "tabler_lm.Rda")

tabler.lm(poly.scaled) %>%
  kable(align = "l") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = TRUE, font_size = 12, position = "center") %>% 
  scroll_box(width = "100%", height = "160px")
```

#### Multicollinearity Concerns

A special note should be made here about the pitfalls of potential **structural multicollinearity** when conducting a polynomial regression such as this one. As a reminder, structural multicollinearity is a high degree of correlation between predictors caused by generation of new predictors based on those already in the model. Note that this is distinct from **data-based multicollinearity**, which is a high degree of correlation amongst predictors that is present within the data set itself. Both forms of multicollinearity introduce instability within regression coefficient estimates and decrease the statistical power of the tests used in the model; thus, steps should be taken to eliminate or mitigate these detrimental effects if observed.  

The potential to introduce a high degree of structural multicollinearity into the model via introduction of the $ratio^2$ term is obvious, as the term in its raw form is directly calculated from the first-order **ratio** term. Fortunately, because this is a textbook example, the solution is well-known and easy to implement: *we center and standardize (or "scale") the predictor levels*. Mathematically, we ensure that *both* predictors are normally distributed such that $ratio \sim (0, 1)$, $ratio^2 \sim (0, 1)$.  
  
The implementation of this solution within R is trivial via usage of the *poly()* function, which transforms the raw data

```{r Raw Ratios}
raw.matrix <- matrix(c(ratio, ratio^2), ncol = 2)
colnames(raw.matrix) <- c("Ratio", "Ratio^2")
raw.matrix
```
  
into   
  
```{r Scaled Ratios}
scaled.matrix <- round(poly(ratio, 2)[ , ], 3)
colnames(scaled.matrix) <- c("Ratio", "Ratio^2")
scaled.matrix
```

which is suitable for usage as predictors in a regression model without fear of the multicollinearity pitfalls mentioned above.  

## Optimization  

The inflection point (the peak) of the polynomial in Eq. 1 can be calculated via means of differentiation  

<center>  

$perf = \beta_{0} + \beta_{1}ratio + \beta_{2}ratio^2$  
$\Rightarrow \frac{dperf}{dratio} = \beta_1 + 2\beta_{2}ratio$  

</center>  

and setting the derivative to zero, and solving for the variable **ratio**:  

<center>  

$\frac{dperf}{dratio} = 0$  
$\Rightarrow \beta_{1} = -2\beta_{2}ratio$  
$\Rightarrow ratio_{crit} = -\frac{\beta_1}{2\beta_{2}}$  

</center>  

The mathematics of this operation must be somewhat modified due
 
## Summary  
  
